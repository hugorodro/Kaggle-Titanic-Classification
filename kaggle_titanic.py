# -*- coding: utf-8 -*-
"""Kaggle-Titanic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v_JTDevMddG-8CxDe7RlGf2-5sVn_9v7
"""

from google.colab import files 
uploaded = files.upload()

import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
from sklearn import preprocessing
import matplotlib.pyplot as plt
import seaborn as sns

#init training dataframe
data_train = pd.read_csv('train.csv')
data_test = pd.read_csv('test.csv')
data_test_y = pd.read_csv('gender_submission.csv')

#chech the state of the data
missing_training_count = data_train.isnull().sum()
print("# of rows in training data = " + str(len(data_train.index)))
print(missing_training_count)

missing_testing_count = data_test.isnull().sum()
print("# of rows in test data = " + str(len(data_test.index)))
print(missing_testing_count)

missing_y_count = data_test_y.isnull().sum()
print("# of rows in test data = " + str(len(data_test_y.index)))
print(missing_y_count)

unique_Embarked=data_train['Embarked'].nunique()
unique_Tickets = data_train['Ticket'].nunique()
print(unique_Embarked)
print(unique_Tickets)

# drop columns: 
## passengerid becuase intuition tells me it is arbitrary
## name because intuition tells me it is arbitrary. Hypothetically there could be a discrimination factor, but that is outside of the scope
## cabin becuase there are too many null values
## ticket because there is too much variety 

data_train_dropped_columns = data_train.drop(['PassengerId', 'Name', 'Cabin', 'Ticket'],axis= 1)
missing_training_count = data_train_dropped_columns.isnull().sum()
print("# of rows in training data = " + str(len(data_train_dropped_columns.index)))
print(missing_training_count)

data_test_dropped_columns = data_test.drop(['PassengerId', 'Name', 'Cabin', 'Ticket'],axis= 1)
missing_testing_count = data_test_dropped_columns.isnull().sum()
print("# of rows in test data = " + str(len(data_test_dropped_columns.index)))
print(missing_testing_count)

data_test_y_dropped_column= data_test_y.drop('PassengerId',axis=1)
missing_y_count = data_test_y_dropped_column.isnull().sum()
print("# of rows in test data = " + str(len(data_test_y_dropped_column.index)))
print(missing_y_count)

data_test_dropped_columns['Survived']= data_test_y_dropped_column
data_test_dropped_columns.head()

#remove the null values
data_train_dropped_nulls = data_train_dropped_columns.dropna(how='any',axis=0)
missing_training_count = data_train_dropped_nulls.isnull().sum()
print("# of rows in training data = " + str(len(data_train_dropped_nulls.index)))
print(missing_training_count)

data_test_dropped_nulls = data_test_dropped_columns.dropna(how='any',axis=0)
missing_testing_count = data_test_dropped_nulls.isnull().sum()
print("# of rows in test data = " + str(len(data_test_dropped_nulls.index)))
print(missing_testing_count)
data_test_dropped_nulls.head()

#set int values for catagories in 'Embarked'
def set_embarked_cat(aDF, index):
  for i in range(0,len(aDF.index)):
    value = aDF.iloc[i,index]
    if  value == "S":
      aDF.iloc[i,index] =1 
    elif value =="C":
      aDF.iloc[i,index] =2 
    elif value =='Q': 
      aDF.iloc[i,index] =3
  return aDF

#set int values for catagories in 'Sex'
def set_sex_cat(aDF, index):
  for i in range(0,len(aDF.index)):
    value = aDF.iloc[i,index]
    if value =='male':
      aDF.iloc[i,index] = 1
    else:
      aDF.iloc[i,index] = 2
  return aDF

data_train_clean = set_embarked_cat(data_train_dropped_nulls,7)
data_train_clean = set_sex_cat(data_train_clean,2)

data_test_clean = set_embarked_cat(data_test_dropped_nulls,6)
data_test_clean = set_sex_cat(data_test_clean,1)

data_test_clean.head(10)

#set the data to be normalized
Y_train = data_train_clean['Survived']
X_train  = data_train_clean.drop('Survived',axis=1)
Y_test = data_test_clean['Survived']
X_test =  data_test_clean.drop('Survived',axis=1)

#actually normalize the data !
train_array = X_train.values
test_array = X_test.values

min_max_scaler = preprocessing.MinMaxScaler()

X_train_scaled = min_max_scaler.fit_transform(train_array)
X_test_scaled = min_max_scaler.fit_transform(test_array)

X_train_normalized = pd.DataFrame(X_train_scaled)
X_test_normalized = pd.DataFrame(X_test_scaled)

log_reg = LogisticRegression()
log_reg.fit(X_train_normalized,Y_train)

predictions = log_reg.predict(X_test_normalized)

score = log_reg.score(X_test_normalized, Y_test)
print(score)

cf = metrics.confusion_matrix(Y_test,predictions)
print(cf)